# Project-Chatgpt-Clone

This code implements a chatbot using the GPT-3.5 Turbo instruct model from OpenAI. The chatbot allows users to input queries and receive responses generated by the model.

## Installation & create environment

Clone the project

```bash
  git clone link_to_copy
```

Go to the project directory

```bash
  cd proj_dir
```

Create the enviroment

```bash
  conda create --prefix ./lang_env
  conda activate {path}/lang_env
  python -m ipykernel install --user --name=lang_env
```

Install dependencies

```bash
  pip install -r requirements.txt
```

Start the server

```bash
  streamlit run app.py
```

## Code Explanation

The code is structured as follows:

Importing the necessary libraries and modules.
Initializing session data for memory.
Setting up the Streamlit page and sidebar.
Defining the main logic for generating responses.
Creating containers for displaying user input and response messages.
Handling user input and generating responses.

Detailed Explanation:

The get_response function takes two parameters: user_input and api_key. It is responsible for generating AI responses based on the user's input using the GPT-3.5 Turbo model.

The code first checks if the conversation object (st.session_state['conversation']) is None. If it is None, it initializes the conversation object by creating an instance of the OpenAI class with the GPT-3.5 Turbo model and the provided API key. It also creates a ConversationChain object to manage the conversation history and a ConversationBufferMemory object to store the memory.

Once the conversation object is initialized, the code calls the predict method of the conversation object, passing the user_input as the input. This method generates a response based on the input and the conversation history.

Finally, the generated response is returned from the function.

Session Data Initialization: The code initializes the session data for conversation, messages, and API key. If these variables are not present in the session state, they are created with default values.

Streamlit Page Configuration: The code sets the page title and displays a heading for the chat interface.

Sidebar Details: The code creates a sidebar where users can enter their API key. There is also a button to start the chat.

User Input and Submission: The code creates a container for the user to input their query and submit it. When the user submits their query, it is added to the conversation history and sent to the AI model for a response.

Response Container: The code creates a container to display the conversation history between the user and the AI model. It iterates through the messages in the session state and displays them in a conversational view.

libraries used:

```
import streamlit as st
from streamlit_chat import message
from langchain.llms import OpenAI
from langchain.memory import ConversationTokenBufferMemory,ConversationBufferMemory,ConversationBufferWindowMemory,ConversationSummaryMemory
from langchain.chains import ConversationChain
```

## Conclusion

The Chat GPT Clone code allows users to have a conversation with an AI model using Streamlit. By inputting their queries and receiving responses, users can interact with the AI model in a conversational manner. The code structure and examples provided demonstrate how the conversation history is maintained and displayed in a user-friendly way.
